{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25138,"status":"ok","timestamp":1735875957339,"user":{"displayName":"Neel Shah","userId":"00917956517875349065"},"user_tz":-330},"id":"jFFNRTyJkYYa","outputId":"4e5925e8-2cb4-42c9-ea17-1da9a9538950"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"FiV18tHu90V1"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3r_RrD-QPQZg"},"outputs":[],"source":["# IMP: Do remove it before sending it to anyone\n","API_KEY = \"\"\n","SOURCE_VIDEO = '/content/drive/MyDrive/Genuin Assignment/single_class_ip/JqNjeNeJzuA.mp4'\n","FRAMES_FOLDER_ADDRESS = \"/content/JqNjeNeJzuA\""]},{"cell_type":"markdown","metadata":{"id":"kTZ1vCjk92db"},"source":["### Before you start\n","\n","Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1735875965265,"user":{"displayName":"Neel Shah","userId":"00917956517875349065"},"user_tz":-330},"id":"U1V7_00F9Yn3","outputId":"d28b10a9-c66a-4145-8719-220cae9e97ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Jan  3 03:46:01 2025       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"wgCy8hAs99ot"},"source":["**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":837,"status":"ok","timestamp":1735875966099,"user":{"displayName":"Neel Shah","userId":"00917956517875349065"},"user_tz":-330},"id":"JpTgINqB94Wt","outputId":"d5060ecb-b1e5-4f23-f19f-1e9b7cd1b19a"},"outputs":[{"name":"stdout","output_type":"stream","text":["HOME: /content\n"]}],"source":["import os\n","HOME = os.getcwd()\n","print(\"HOME:\", HOME)"]},{"cell_type":"markdown","metadata":{"id":"UnK9Ad88-CGR"},"source":["### Install SAM2 and dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59816,"status":"ok","timestamp":1735876343422,"user":{"displayName":"Neel Shah","userId":"00917956517875349065"},"user_tz":-330},"id":"9WI-Of6r9_tQ","outputId":"5dbd66e5-3789-4f2d-815b-7e05e1e4f35d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'segment-anything-2'...\n","remote: Enumerating objects: 1070, done.\u001b[K\n","remote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\u001b[K\n","Receiving objects: 100% (1070/1070), 134.70 MiB | 35.20 MiB/s, done.\n","Resolving deltas: 100% (376/376), done.\n","/content/segment-anything-2\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","running build_ext\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","  warnings.warn(msg.format('we could not find ninja.'))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n","  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n","  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n","building 'sam2._C' extension\n","creating build/temp.linux-x86_64-cpython-310/sam2/csrc\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n","If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n","  warnings.warn(\n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c sam2/csrc/connected_components.cu -o build/temp.linux-x86_64-cpython-310/sam2/csrc/connected_components.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n","creating build/lib.linux-x86_64-cpython-310/sam2\n","x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions build/temp.linux-x86_64-cpython-310/sam2/csrc/connected_components.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/sam2/_C.so\n","copying build/lib.linux-x86_64-cpython-310/sam2/_C.so -> sam2\n","Collecting anthropic\n","  Downloading anthropic-0.42.0-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.8.2)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.10.3)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.12.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.1)\n","Downloading anthropic-0.42.0-py3-none-any.whl (203 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: anthropic\n","Successfully installed anthropic-0.42.0\n"]}],"source":["!git clone https://github.com/facebookresearch/segment-anything-2.git\n","%cd {HOME}/segment-anything-2\n","!pip install -e . -q\n","!python setup.py build_ext --inplace\n","!pip install anthropic"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8258,"status":"ok","timestamp":1735876351678,"user":{"displayName":"Neel Shah","userId":"00917956517875349065"},"user_tz":-330},"id":"YbZOliEl-Hyz","outputId":"0c00d471-6918-4af5-a207-60720bbe6b5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: supervision 0.25.1 does not provide the extra 'assets'\u001b[0m\u001b[33m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.7/213.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.4/727.4 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q supervision[assets] jupyter_bbox_widget"]},{"cell_type":"markdown","metadata":{"id":"V-50ZjJA-MPl"},"source":["### Download SAM2 checkpoints\n","\n","**NOTE:** SAM2 is available in 4 different model sizes ranging from the lightweight \"sam2_hiera_tiny\" (38.9M parameters) to the more powerful \"sam2_hiera_large\" (224.4M parameters)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Tgf2WKiO-ECI"},"outputs":[],"source":["!mkdir -p {HOME}/checkpoints\n","!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt -P {HOME}/checkpoints\n","!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt -P {HOME}/checkpoints\n","!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt -P {HOME}/checkpoints\n","!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P {HOME}/checkpoints"]},{"cell_type":"markdown","metadata":{"id":"EbD4Hv0q-Rmt"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-HL-09Fj-OdY"},"outputs":[],"source":["import cv2\n","import torch\n","import base64\n","\n","import numpy as np\n","import supervision as sv\n","\n","from pathlib import Path\n","from supervision.assets import download_assets, VideoAssets\n","from sam2.build_sam import build_sam2_video_predictor\n","\n","import cv2\n","import base64\n","from anthropic import Anthropic\n","import json\n","import re\n","import matplotlib.pyplot as plt\n","import json\n","\n","IS_COLAB = True\n","\n","if IS_COLAB:\n","    from google.colab import output\n","    output.enable_custom_widget_manager()\n","\n","from jupyter_bbox_widget import BBoxWidget"]},{"cell_type":"markdown","metadata":{"id":"GYQV0xYu-W16"},"source":["**NOTE:** This code enables mixed-precision computing for faster deep learning. It uses bfloat16 for most calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for certain operations to further boost performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_ca4kbz-UlP"},"outputs":[],"source":["torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n","\n","if torch.cuda.get_device_properties(0).major >= 8:\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True"]},{"cell_type":"markdown","metadata":{"id":"b2a5NdUA_drI"},"source":["## Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oXn6kAwP-YyE"},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","CHECKPOINT = f\"{HOME}/checkpoints/sam2_hiera_large.pt\"\n","CONFIG = \"sam2_hiera_l.yaml\"\n","\n","sam2_model = build_sam2_video_predictor(CONFIG, CHECKPOINT)"]},{"cell_type":"markdown","metadata":{"id":"HPrepIFqBJQA"},"source":["## Preprocess video"]},{"cell_type":"markdown","metadata":{"id":"-u7r97KoBNmG"},"source":["### Download video and split it into frames\n","\n","**NOTE:** SAM2 assumee that the video is stored as a list of JPEG frames with filenames like `<frame_index>.jpg`. Let's start by downloading a sample video, splitting it into frames, and saving them to disk. Feel free to replace `SOURCE_VIDEO` with the path to your video file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tjn6hQpkQMcY"},"outputs":[],"source":["# SOURCE_VIDEO = download_assets(VideoAssets.BASKETBALL)\n","# SOURCE_VIDEO = '/content/drive/MyDrive/tmp_proj/double_class_ip/S3r4CFtv9uU.mp4'\n","\n","obj = sv.VideoInfo.from_video_path(SOURCE_VIDEO)\n","width, height, fps, total_frames = obj.width, obj.height, obj.fps, obj.total_frames\n","print(width, height, fps, total_frames)"]},{"cell_type":"markdown","metadata":{"id":"5NAOFsuzQV0m"},"source":["**NOTE:** To reduce VRAM requirements, we are introducing three additional parameters: `SCALE_FACTOR` to decrease the frame resolution, and `START_IDX` and `END_IDX` to extract only the relevant segments from the video."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4zahf7rQfTU"},"outputs":[],"source":["# Do change the START_IDX, END_IDX and also the frame id which is used for the annotation\n","\n","SCALE_FACTOR = 0.5 # Have to experiment with it by removing it\n","START_IDX = 100 # Also, have to manage both of them\n","END_IDX = 300"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u83ozL_n_24I"},"outputs":[],"source":["SOURCE_FRAMES = Path(HOME) / Path(SOURCE_VIDEO).stem\n","SOURCE_FRAMES.mkdir(parents=True, exist_ok=True)\n","\n","frames_generator = sv.get_video_frames_generator(SOURCE_VIDEO, start=START_IDX, end=END_IDX)\n","images_sink = sv.ImageSink(\n","    target_dir_path=SOURCE_FRAMES.as_posix(),\n","    overwrite=True,\n","    image_name_pattern=\"{:05d}.jpeg\"\n",")\n","\n","with images_sink:\n","    for frame in frames_generator:\n","        frame = sv.scale_image(frame, SCALE_FACTOR)\n","        images_sink.save_image(frame)\n","\n","TARGET_VIDEO = Path(HOME) / f\"{Path(SOURCE_VIDEO).stem}-result.mp4\"\n","SOURCE_FRAME_PATHS = sorted(sv.list_files_with_extensions(SOURCE_FRAMES.as_posix(), extensions=[\"jpeg\"]))"]},{"cell_type":"markdown","metadata":{"id":"LOnaA1rtF_mQ"},"source":["### Initialize the inference state\n","\n","**NOTE:** SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an inference state on this video. During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EEbzUqQEeg9"},"outputs":[],"source":["inference_state = sam2_model.init_state(video_path=SOURCE_FRAMES.as_posix())"]},{"cell_type":"markdown","metadata":{"id":"-jws6IbnHCYp"},"source":["**NOTE:** If you have run any previous tracking using this inference_state, please reset it first via reset_state. (The cell below is just for illustration; it's not needed to call reset_state here as this inference_state is just freshly initialized above.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tVN_2oaGp28"},"outputs":[],"source":["sam2_model.reset_state(inference_state)"]},{"cell_type":"markdown","metadata":{"id":"8jGsiYlpHbsY"},"source":["### Prompting with points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vy133frIHIPF"},"outputs":[],"source":["def encode_image(filepath):\n","    with open(filepath, 'rb') as f:\n","        image_bytes = f.read()\n","    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n","    return \"data:image/jpg;base64,\"+encoded"]},{"cell_type":"markdown","metadata":{"id":"b29AoCORHnoD"},"source":["**NOTE:** SAM2 allows tracking multiple objects at once. Update the `OBJECTS` list if you want to change the list of tracked objects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwhQeJBYPZXs"},"outputs":[],"source":["num_images = 5\n","\n","def analyze_video_frames_unique(video_path, api_key):\n","    client = Anthropic(api_key=api_key)\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    interval = total_frames // num_images\n","\n","    frames_base64 = []\n","    for i in range(num_images):\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i * interval)\n","        ret, frame = cap.read()\n","        if ret:\n","            _, buffer = cv2.imencode('.jpg', frame)\n","            frames_base64.append(base64.b64encode(buffer).decode('utf-8'))\n","\n","    cap.release()\n","\n","    messages = [{\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\n","            \"type\": \"text\",\n","            \"text\": \"\"\"Return ONLY a JSON array with exactly 2 items containing unique main objects from these frames, prioritizing humans or living beings if present. Rules:\n","            1. Format: [\"Object1\", \"Object2\"] or [\"Object1\", null]\n","            2. Maximum 2 unique objects\n","            3. Focus on humans or living beings first before considering non-living objects.\n","            4. No explanation text, just the JSON array\"\"\"\n","        }\n","        ]\n","    }]\n","\n","    for img_base64 in frames_base64:\n","        messages[0][\"content\"].append({\n","            \"type\": \"image\",\n","            \"source\": {\n","                \"type\": \"base64\",\n","                \"media_type\": \"image/jpeg\",\n","                \"data\": img_base64\n","            }\n","        })\n","\n","    response = client.messages.create(\n","        model=\"claude-3-haiku-20240307\",\n","        max_tokens=150,\n","        messages=messages\n","    )\n","\n","    # Extract JSON array using regex\n","    json_match = re.search(r'\\[.*?\\]', response.content[0].text)\n","    if json_match:\n","        return json.loads(json_match.group())\n","    return [None, None]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sq6J4HI0PbqD"},"outputs":[],"source":["api_key = API_KEY\n","video_path = SOURCE_VIDEO\n","unique_objects = analyze_video_frames_unique(video_path, api_key)\n","print(unique_objects)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8AjTRQ8TBsS"},"outputs":[],"source":["OBJECTS = unique_objects"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-iDpn0RHMvSZ"},"outputs":[],"source":["# Fix the claude API, giving not probable things (doing tmp fixing)\n","OBJECTS = ['Spider-Man.']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsMbT_hXben7"},"outputs":[],"source":["# def make_pixel_grid_with_lines(image_path, output_path):\n","#     # Load the image\n","#     image = cv2.imread(image_path)\n","#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","#     # Add grid lines and pixel labels\n","#     font = cv2.FONT_HERSHEY_SIMPLEX\n","#     font_scale = 0.7\n","#     color = (255, 255, 255)  # White color in RGB\n","#     thickness = 2\n","\n","#     step = 100  # Distance between lines\n","\n","#     for x in range(0, image.shape[1], step):  # Vertical lines\n","#         cv2.line(image, (x, 0), (x, image.shape[0] - 1), color, thickness)\n","#         # cv2.putText(image, f'{x}', (x + 5, 15), font, font_scale, color, thickness)\n","\n","#     for y in range(0, image.shape[0], step):  # Horizontal lines\n","#         cv2.line(image, (0, y), (image.shape[1] - 1, y), color, thickness)\n","#         # cv2.putText(image, f'{y}', (5, y + 15), font, font_scale, color, thickness)\n","\n","#     # Display the image with matplotlib and save with tight layout\n","#     plt.figure(figsize=(10, 10))\n","#     plt.imshow(image)\n","#     plt.axis('on')  # Show axes with pixel values\n","#     plt.grid(False)  # Disable Matplotlib's default grid\n","\n","#     # Save the image without extra white space\n","#     plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n","#     plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeeqUPduxUj7"},"outputs":[],"source":["# def extract_coordinates(response, num_objects):\n","#     try:\n","#         # Extract all coordinates from the response using regex\n","#         coordinates = re.findall(r\"\\((\\d+),\\s*(\\d+)\\)\", response)\n","#         coordinates = [(int(x), int(y)) for x, y in coordinates]\n","\n","#         # If the number of extracted coordinates matches the expected number, return them\n","#         if len(coordinates) == num_objects:\n","#             return coordinates\n","\n","#         # If more coordinates are provided, group and select the central one for each object\n","#         elif len(coordinates) > num_objects:\n","#             # Calculate the central coordinate\n","#             center_x = np.mean([x for x, y in coordinates])\n","#             center_y = np.mean([y for x, y in coordinates])\n","\n","#             # Sort by distance to the central point\n","#             coordinates_sorted = sorted(\n","#                 coordinates,\n","#                 key=lambda coord: np.sqrt((coord[0] - center_x) ** 2 + (coord[1] - center_y) ** 2)\n","#             )\n","\n","#             # Return the first 'num_objects' coordinates\n","#             return coordinates_sorted[:num_objects]\n","\n","#         else:\n","#             raise ValueError(\"Not enough coordinates extracted for the specified number of objects.\")\n","\n","#     except Exception as e:\n","#         print(f\"Error: Could not parse coordinates from response\\n{e}\")\n","#         return []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81m99-Irdzgy"},"outputs":[],"source":["# def analyze_image_coordinates(api_key, image_path, objects_to_find):\n","#     client = Anthropic(api_key=api_key)\n","\n","#     mime_type = 'image/jpeg'  # Default to PNG if can't detect\n","\n","#     # Read and encode image\n","#     with open(image_path, \"rb\") as f:\n","#         image_bytes = f.read()\n","#         image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n","\n","#     prompt = f\"I have an image containing specific objects. The image is divided into a grid with labeled coordinates for both x and y axes. Please identify the an point (x, y) coordinates of the specified objects in the image, make sure that the point lies in that object not at extreme end of object. For this image, the objects to locate are [{', '.join(objects_to_find)}]. Provide the coordinates for each object based on the grid.\"\n","\n","#     try:\n","#         message = client.messages.create(\n","#             model=\"claude-3-haiku-20240307\",\n","#             max_tokens=1000,\n","#             messages=[{\n","#                 \"role\": \"user\",\n","#                 \"content\": [\n","#                     {\n","#                         \"type\": \"text\",\n","#                         \"text\": prompt\n","#                     },\n","#                     {\n","#                         \"type\": \"image\",\n","#                         \"source\": {\n","#                             \"type\": \"base64\",\n","#                             \"media_type\": mime_type,\n","#                             \"data\": image_base64\n","#                         }\n","#                     }\n","#                 ]\n","#             }]\n","#         )\n","\n","#         return extract_coordinates(message.content[0].text, len(objects_to_find))\n","\n","#     except json.JSONDecodeError:\n","#         print(\"Raw response:\", message.content[0].text)\n","#         return \"Error: Could not parse coordinates from response\"\n","#     except Exception as e:\n","#         print(f\"Error: {str(e)}\")\n","#         return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMODOLlLHlct"},"outputs":[],"source":["# FRAME_IDX = 0\n","# FRAME_PATH = Path(SOURCE_FRAMES) / f\"{FRAME_IDX:05d}.jpeg\"\n","# ANNOTED_FRAME_PATH = Path(SOURCE_FRAMES) / f\"{FRAME_IDX:05d}-annotated.jpeg\"\n","\n","# make_pixel_grid_with_lines(image_path = FRAME_PATH, output_path=ANNOTED_FRAME_PATH)\n","\n","# coordinates = analyze_image_coordinates(api_key=API_KEY, image_path=ANNOTED_FRAME_PATH, objects_to_find=unique_objects)\n","# print(coordinates)"]},{"cell_type":"markdown","metadata":{"id":"wDRQAwfRs9m-"},"source":["##LOWWER SETUP REMAINING"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkA_jl4nqs-V"},"outputs":[],"source":["import requests\n","import torch\n","from PIL import Image\n","import numpy as np\n","from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n","\n","model_id_dino = \"IDEA-Research/grounding-dino-tiny\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","processor_dino = AutoProcessor.from_pretrained(model_id_dino)\n","model_dino = AutoModelForZeroShotObjectDetection.from_pretrained(model_id_dino).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fEdQR1tqvxB"},"outputs":[],"source":["def calculate_bounding_box_areas_dino(bounding_boxes):\n","    areas = []\n","    for box in bounding_boxes:\n","      if len(box) == 0:\n","        areas.append(0)\n","      else:\n","        x1, y1, x2, y2 = box\n","\n","        # Calculate width and height of the bounding box\n","        width = abs(x2 - x1)\n","        height = abs(y2 - y1)\n","\n","        # Calculate area\n","        area = width * height\n","        areas.append(area // 1000)\n","\n","    return areas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlpfibuDsZmZ"},"outputs":[],"source":["def get_bounding_boxes_dino(image_path, text=\"a spyder man.\"):\n","  image = Image.open(image_path).convert('RGB')\n","\n","  # Text prompt for detection\n","  # text = \"a spyder man.\"\n","\n","  # Process inputs\n","  inputs = processor_dino(images=image, text=text, return_tensors=\"pt\").to(device)\n","\n","  # Run inference\n","  with torch.no_grad():\n","      outputs = model_dino(**inputs)\n","\n","  # Post-process results\n","  results = processor_dino.post_process_grounded_object_detection(\n","      outputs,\n","      inputs.input_ids,\n","      box_threshold=0.4,\n","      text_threshold=0.3,\n","      target_sizes=[image.size[::-1]]\n","  )[0]  # Get first (and only) image results\n","\n","  boxes = []\n","\n","  if len(results['boxes']) == 0:\n","      return [], 0\n","\n","  # Print results in a more readable format\n","  for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n","    box = [round(i, 2) for i in box.tolist()]\n","\n","\n","    if len(results) <= 1:\n","      return box, calculate_bounding_box_areas_dino(box)[0]\n","    else:\n","      # Do the max thing\n","      boxes.append(box)\n","\n","  areas = calculate_bounding_box_areas_dino(boxes)\n","  index_of_max = max(range(len(areas)), key=lambda i: areas[i])\n","  return boxes[index_of_max], areas[index_of_max]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLsqPIo5sZjl"},"outputs":[],"source":["def calculate_center_dino(box):\n","  if len(box) == 0:\n","    return (0, 0)\n","  x1, y1, x2, y2 = box\n","\n","  # Calculate width and height of the bounding box\n","  width = abs(x2 - x1)\n","  height = abs(y2 - y1)\n","\n","  # Calculate area\n","  area = width * height\n","\n","  # Calculate center\n","  center_x = int((x1 + x2) / 2)\n","  center_y = int((y1 + y2) / 2)\n","  return (center_x, center_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ojPkpz1sZhP"},"outputs":[],"source":["def get_max_info_frame_dino(image_paths, texts):\n","    if isinstance(texts, str):\n","        texts = [texts]\n","\n","    cumulative_areas = []\n","    all_boxes = []\n","    all_centers = []\n","\n","    for img in image_paths:\n","        total_area = 0\n","        best_box = []\n","        best_center = []\n","        for text in texts:\n","            box, area = get_bounding_boxes_dino(img, text)\n","            if box is not None:\n","                total_area += area\n","                center = calculate_center_dino(box)\n","\n","                best_box.append(box)\n","                best_center.append(center)\n","            else:\n","                best_box.append([])\n","                best_center.append([])\n","\n","        cumulative_areas.append(total_area)\n","        all_boxes.append(best_box if best_box else [])\n","        all_centers.append(best_center if best_center else [])\n","\n","    index_of_max = max(range(len(cumulative_areas)), key=lambda i: cumulative_areas[i])\n","    return image_paths[index_of_max], all_boxes[index_of_max], all_centers[index_of_max]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSXwb0xzyAM3"},"outputs":[],"source":["def get_frame_id(file_path):\n","    \"\"\"\n","    Extracts the frame ID (number before the file extension) from a file path.\n","\n","    Parameters:\n","        file_path (str): The file path as a string.\n","\n","    Returns:\n","        str: The frame ID as a string.\n","    \"\"\"\n","    # Extract the base name of the file (e.g., '00100.jpeg')\n","    file_name = os.path.basename(file_path)\n","\n","    # Split the file name into the name and extension\n","    frame_id = os.path.splitext(file_name)[0]\n","\n","    return frame_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxZqdNFmuurM"},"outputs":[],"source":["import os\n","\n","# Define the source folder\n","source_folder = FRAMES_FOLDER_ADDRESS  # Update this to your folder path\n","\n","# Get a sorted list of all image files in the source folder\n","image_files = sorted(\n","    [f for f in os.listdir(source_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",")\n","\n","# Determine the interval to pick 10 frames evenly\n","total_frames = len(image_files)\n","if total_frames < 10:\n","    raise ValueError(\"Not enough frames to select 10 different frames.\")\n","\n","interval = total_frames // 10\n","\n","# Generate the list of paths for the selected frames\n","selected_frames = [\n","    os.path.join(source_folder, image_files[i * interval])\n","    for i in range(10)\n","]\n","\n","# Print or use the list of selected frames\n","print(\"Selected frame paths:\")\n","for frame in selected_frames:\n","    print(frame)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8P-pmy1sfwT"},"outputs":[],"source":["# TODO:Do Check the full stop thing in the prompt\n","\n","frame_path, boxes, centers = get_max_info_frame_dino(image_paths=selected_frames, texts=['Spider-Man.'])\n","print(frame_path, boxes, centers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UukGp555xfLr"},"outputs":[],"source":["FRAME_IDX = int(get_frame_id(frame_path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3ouBAD3xHw7"},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","\n","def paint_point_on_image(image_path, x, y, output_path=None, point_color=(255, 0, 0), point_radius=12):\n","    import cv2\n","    import matplotlib.pyplot as plt\n","\n","    # Load the image\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        print(\"Error: Could not load image.\")\n","        return\n","\n","    # Paint the point on the image\n","    cv2.circle(image, (x, y), radius=point_radius, color=point_color, thickness=-1)\n","\n","    # Convert to RGB for displaying in matplotlib\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Display the image with matplotlib\n","    plt.imshow(image_rgb)\n","    plt.axis('on')\n","    plt.title(f\"Point painted at ({x}, {y})\")\n","    plt.show()\n","\n","    # Save the image if an output path is provided\n","    # if output_path:\n","    #     cv2.imwrite(output_path, image)\n","\n","paint_point_on_image(frame_path, centers[0][0], centers[0][1])"]},{"cell_type":"markdown","metadata":{"id":"8zDXqv17s5LV"},"source":["##UPPER SETUP REMAINING"]},{"cell_type":"markdown","metadata":{"id":"5KGcLONsTzEl"},"source":["**NOTE:** Let's choose the index of the reference frame that we will use to annotate the objects we are looking for."]},{"cell_type":"markdown","metadata":{"id":"QfLT4ybLWGPO"},"source":["**NOTE:** The widget we are using stores annotations in a format that is inconsistent with SAM2's requirements. We parse them and then pass them to SAM2 via the `add_new_points` method. Each of the objects we track must be passed via a separate `add_new_points` call. It is important to specify `frame_idx` each time - the index of the frame to which the annotations relate, and `obj_id` - the ID of the object to which the annotations relate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-3jRItUXw3r"},"outputs":[],"source":["# # TODO\n","\n","# widget = BBoxWidget(classes=OBJECTS)\n","# widget.image = encode_image(FRAME_PATH)\n","# widget.classes = unique_objects\n","# widget.bboxes = [{'x': x_axis, 'y': y_axis, 'width': 0, 'height': 0, 'label': label_name} for (x_axis, y_axis), label_name in zip(coordinates, unique_objects)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0emMKUJPttq5"},"outputs":[],"source":["# # TODO\n","\n","widget = BBoxWidget(classes=OBJECTS)\n","widget.image = encode_image(frame_path)\n","widget.classes = OBJECTS\n","widget.bboxes = [{'x': x_axis, 'y': y_axis, 'width': 0, 'height': 0, 'label': label_name} for (x_axis, y_axis), label_name in zip(centers, unique_objects)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeAfOMoN04TD"},"outputs":[],"source":["print(widget)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpz4oKDDIevv"},"outputs":[],"source":["# default_box = [\n","#     {'x': 705, 'y': 302, 'width': 0, 'height': 0, 'label': 'ball'},\n","#     {'x': 587, 'y': 300, 'width': 0, 'height': 0, 'label': 'player-1'},\n","#     {'x': 753, 'y': 267, 'width': 0, 'height': 0, 'label': 'player-2'}\n","# ]\n","# boxes = widget.bboxes if widget.bboxes else default_box\n","# boxes = default_box\n","\n","for object_id, label in enumerate(OBJECTS, start=1):\n","    boxes = [box for box in widget.bboxes if box['label'] == label]\n","\n","    if len(boxes) == 0:\n","        continue\n","\n","    points = np.array([\n","        [\n","            box['x'],\n","            box['y']\n","        ] for box in boxes\n","    ], dtype=np.float32)\n","    labels = np.ones(len(points))\n","\n","    _, object_ids, mask_logits = sam2_model.add_new_points(\n","        inference_state=inference_state,\n","        frame_idx=FRAME_IDX,\n","        obj_id=object_id,\n","        points=points,\n","        labels=labels,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"-nF6htDYa8aS"},"source":["### Video inference\n","\n","**NOTE:** To apply our point prompts to all video frames, we use the `propagate_in_video` generator. Each call returns `frame_idx` - the index of the current frame, `object_ids` - IDs of objects detected in the frame, and `mask_logits` - corresponding `object_ids` logit values, which we can convert to masks using thresholding."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"u3xN2ATbTyqD","outputId":"f1f9c318-2eea-4230-bccd-d677f0106e04"},"outputs":[{"name":"stderr","output_type":"stream","text":["propagate in video:   0%|          | 0/100 [00:00<?, ?it/s]\n"]},{"ename":"ValueError","evalue":"not enough values to unpack (expected 2, got 1)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-73-a6763b17c1d1>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         detections = sv.Detections(\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mxyxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_to_xyxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mclass_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervision/detection/utils.py\u001b[0m in \u001b[0;36mmask_to_xyxy\u001b[0;34m(masks)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"]}],"source":["video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO)\n","video_info.width = int(video_info.width * SCALE_FACTOR)\n","video_info.height = int(video_info.height * SCALE_FACTOR)\n","\n","COLORS = ['#FF1493', '#00BFFF', '#FF6347', '#FFD700']\n","mask_annotator = sv.MaskAnnotator(\n","    color=sv.ColorPalette.from_hex(COLORS),\n","    color_lookup=sv.ColorLookup.CLASS)\n","\n","frame_sample = []\n","detections_list = []\n","\n","with sv.VideoSink(TARGET_VIDEO.as_posix(), video_info=video_info) as sink:\n","    for frame_idx, object_ids, mask_logits in sam2_model.propagate_in_video(inference_state):\n","        frame_path = SOURCE_FRAME_PATHS[frame_idx]\n","        frame = cv2.imread(frame_path)\n","        masks = (mask_logits > 0.0).cpu().numpy()\n","        masks = np.squeeze(masks).astype(bool)\n","\n","        detections = sv.Detections(\n","            xyxy=sv.mask_to_xyxy(masks=masks),\n","            mask=masks,\n","            class_id=np.array(object_ids)\n","        )\n","        detections_list.append(sv.mask_to_xyxy(masks=masks))\n","\n","        annotated_frame = mask_annotator.annotate(scene=frame.copy(), detections=detections)\n","\n","        sink.write_frame(annotated_frame)\n","        if frame_idx % video_info.fps == 0:\n","            frame_sample.append(annotated_frame)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bgmAGORZJ7z"},"outputs":[],"source":["sv.plot_images_grid(\n","    images=frame_sample[:4],\n","    grid_size=(2, 2)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtSWdxScp2II"},"outputs":[],"source":["# File path for saving\n","file_path = \"/content/detections.txt\"\n","\n","# Save the detections list to a text file\n","with open(file_path, \"w\") as file:\n","    for detection in detections_list:\n","        file.write(str(detection) + \"\\n\")\n","\n","print(f\"Detections saved to {file_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCHSZ3GFjEJS"},"outputs":[],"source":["def calculate_916_crop(bounding_boxes, image_width, image_height):\n","    \"\"\"\n","    Calculate a consistent 9:16 crop that maintains focus on the center of primary or large objects,\n","    ensuring the crop stays within the image boundaries.\n","\n","    Args:\n","        bounding_boxes: Array or list of [x1, y1, x2, y2] coordinates, ordered by importance\n","        image_width: Original image width\n","        image_height: Original image height\n","\n","    Returns:\n","        [x1, y1, x2, y2] coordinates for 9:16 crop\n","    \"\"\"\n","    # Define 9:16 target ratio\n","    target_ratio = 9 / 16\n","\n","    # Determine fixed crop dimensions based on video size\n","    if image_width / image_height > target_ratio:\n","        # Height is limiting factor\n","        crop_height = image_height\n","        crop_width = crop_height * target_ratio\n","    else:\n","        # Width is limiting factor\n","        crop_width = image_width\n","        crop_height = crop_width / target_ratio\n","\n","    # Add padding if desired (e.g., 10%)\n","    padding_factor = 1.1\n","    crop_width *= padding_factor\n","    crop_height *= padding_factor\n","\n","    # Ensure crop doesn't exceed image dimensions\n","    crop_width = min(crop_width, image_width)\n","    crop_height = min(crop_height, image_height)\n","\n","    # Handle bounding_boxes\n","    if bounding_boxes is None or len(bounding_boxes) == 0:\n","        center_x, center_y = image_width / 2, image_height / 2\n","    else:\n","        # Convert to list if it's a NumPy array\n","        if isinstance(bounding_boxes, np.ndarray):\n","            bounding_boxes = bounding_boxes.tolist()\n","\n","        # Find the largest object (by area) in the bounding boxes\n","        largest_box = max(\n","            bounding_boxes,\n","            key=lambda box: (box[2] - box[0]) * (box[3] - box[1])\n","        )\n","        # Center the crop on the largest object's center\n","        center_x = (largest_box[0] + largest_box[2]) / 2\n","        center_y = (largest_box[1] + largest_box[3]) / 2\n","\n","    # Calculate initial crop coordinates\n","    x1 = max(0, center_x - crop_width / 2)\n","    y1 = max(0, center_y - crop_height / 2)\n","    x2 = x1 + crop_width\n","    y2 = y1 + crop_height\n","\n","    # Adjust crop if it exceeds boundaries\n","    if x2 > image_width:\n","        x2 = image_width\n","        x1 = x2 - crop_width\n","    if y2 > image_height:\n","        y2 = image_height\n","        y1 = y2 - crop_height\n","    if x1 < 0:\n","        x1 = 0\n","        x2 = crop_width\n","    if y1 < 0:\n","        y1 = 0\n","        y2 = crop_height\n","\n","    # Ensure crop coordinates stay within the image boundaries\n","    x1 = max(0, x1)\n","    y1 = max(0, y1)\n","    x2 = min(image_width, x2)\n","    y2 = min(image_height, y2)\n","\n","    return [int(x1), int(y1), int(x2), int(y2)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bngy-x6tVGCy"},"outputs":[],"source":["import cv2\n","\n","def process_video_with_boxes(video_path, detections_list, output_path):\n","    cap = cv2.VideoCapture(video_path)\n","    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n","\n","    frame_count = 0\n","    color = (0, 255, 0)\n","    thickness = 2\n","\n","    while cap.isOpened():  # Corrected this line\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        if frame_count < len(detections_list):\n","            boxes = detections_list[frame_count]\n","            # print(boxes, type(boxes))\n","            crop_coords = calculate_916_crop(boxes, width, height)\n","            cv2.rectangle(frame,\n","                         (crop_coords[0], crop_coords[1]),\n","                         (crop_coords[2], crop_coords[3]),\n","                         color, thickness)\n","\n","        out.write(frame)\n","        frame_count += 1\n","\n","    cap.release()\n","    out.release()\n","\n","# Example usage\n","video_path = f\"{FRAMES_FOLDER_ADDRESS}-result.mp4\"\n","output_path = f\"{FRAMES_FOLDER_ADDRESS}-segmented-rectangle-output.mp4\"\n","\n","# Make sure `detections_list` and `calculate_916_crop` are properly defined\n","process_video_with_boxes(video_path, detections_list, output_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVOnl0qNj8nM"},"outputs":[],"source":["import cv2\n","\n","def process_video_with_boxes(video_path, detections_list, output_path, output_width=1080, output_height=1920):\n","    cap = cv2.VideoCapture(video_path)\n","    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter(output_path, fourcc, fps, (output_width, output_height))\n","\n","    frame_count = 0\n","    color = (0, 255, 0)\n","    thickness = 2\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        if frame_count < len(detections_list):\n","            boxes = detections_list[frame_count]\n","            # Get the coordinates for cropping the frame\n","            crop_coords = calculate_916_crop(boxes, width, height)\n","\n","            # Crop the frame using the coordinates (adjusted to 9:16 aspect ratio)\n","            cropped_frame = frame[crop_coords[1]:crop_coords[3], crop_coords[0]:crop_coords[2]]\n","\n","            # Resize the cropped frame to the desired 9:16 aspect ratio (output_width x output_height)\n","            cropped_frame_resized = cv2.resize(cropped_frame, (output_width, output_height))\n","\n","            # Write the resized cropped frame to the output video\n","            out.write(cropped_frame_resized)\n","\n","        frame_count += 1\n","\n","    cap.release()\n","    out.release()\n","\n","# Example usage\n","video_path = f\"{FRAMES_FOLDER_ADDRESS}-result.mp4\" # TODO: Change this with the original video, or the video without segmentation\n","output_path = f\"{FRAMES_FOLDER_ADDRESS}-final-output.mp4\"\n","\n","# Ensure `detections_list` and `calculate_916_crop` are properly defined\n","process_video_with_boxes(video_path, detections_list, output_path, output_width=1080, output_height=1920)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5EpjE79loO1"},"outputs":[],"source":["# Do add the voice support\n","\n","# Change the souce path\n","\n","# Chnage the indexes\n","\n","# proper frame for the initialization\n","\n","# take the name of the image which have all the objects from the claude only\n","\n","# What happens if there is nothing to segment in the frame, then consider the last choice of the segment and continue with it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wq2mjWdz4sk1"},"outputs":[],"source":["# While taking the subject, instead take a little description of the subject"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ygq5Y844ydK9"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}